# 从零构建DBT情感咨询聊天机器人 - 第一阶段：模型的训练和量化

> **作者：Alex** | **审核：Mike** | **团队：米醋电子工作室**
>
> **版权所有 © L定远**
>
> **最后更新：2026-02-18**

---

## 目录

1. [第一步：Windows环境下大模型训练环境搭建](#第一步windows环境下大模型训练环境搭建)
2. [第二步：如何为情感咨询聊天机器人选择合适的模型](#第二步如何为情感咨询聊天机器人选择合适的模型)
3. [第三步：如何选择情感咨询训练数据集](#第三步如何选择情感咨询训练数据集)
4. [第四步：数据集生成与清洗](#第四步数据集生成与清洗)
5. [第五步：模型训练](#第五步模型训练)
6. [第六步：模型导出与部署](#第六步模型导出与部署)
7. [第七步：模型量化](#第七步模型量化)
8. [第八步：可选方案 - 训练更大的模型](#第八步可选方案---训练更大的模型)
9. [附录：常见问题与解决方案](#附录常见问题与解决方案)

---

## 第一步：Windows环境下大模型训练环境搭建

### 本步目标

在Windows系统上搭建一个大模型训练环境，为后续的模型微调做好准备。

### 前置条件

- Windows 10/11 操作系统
- NVIDIA显卡（推荐RTX 3060及以上）
- 至少100GB可用存储空间

### 硬件与软件环境

#### 硬件配置

| 组件 | 规格 |
|------|------|
| GPU | NVIDIA RTX 3060 (6GB显存) |
| CPU | Intel/AMD 多核处理器 |
| 内存 | 16GB及以上 |
| 存储 | 至少100GB可用空间(SSD优先) |

#### 软件环境

| 软件 | 版本要求 | 说明 |
|------|----------|------|
| Python | 3.10 | 推荐3.10版本（兼容性最佳） |
| CUDA | 11.8+ | GPU计算框架 |
| PyTorch | 2.5+ | 深度学习框架 |

### 环境搭建步骤

#### 第一步：安装Python 3.10

从 [Python官网](https://www.python.org/downloads/) 下载Python 3.10，安装时勾选"Add Python to PATH"。

验证安装：
```powershell
python --version
# 输出：Python 3.10.x
```

#### 第二步：创建Python虚拟环境

```powershell
# 创建虚拟环境
py -3.10 -m venv .venv310

# 激活虚拟环境
.venv310\Scripts\activate
```

#### 第三步：安装PyTorch（CUDA版本）

**重要**：必须安装CUDA版本才能使用GPU训练！

```powershell
# 方法一：直接安装（需要联网）
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 方法二：离线安装（推荐）
# 1. 从 https://download.pytorch.org/whl/cu118/torch/ 下载whl文件
# 2. 安装
pip install torch-2.5.1+cu118-cp310-cp310-win_amd64.whl
```

验证CUDA：
```python
import torch
print("CUDA可用:", torch.cuda.is_available())
print("GPU设备:", torch.cuda.get_device_name(0))
```

#### 第四步：安装训练框架

```powershell
pip install llamafactory
pip install transformers accelerate peft datasets
```

### 训练框架选择

#### LLaMA-Factory

LLaMA-Factory是一个开源的大模型训练框架，具有以下特点：

- 支持多种微调方式：LoRA、QLoRA、Full-tune等
- 提供Web图形界面
- 内置模型下载和转换功能
- 支持国内外主流大模型

### 注意事项

1. **Python版本**：推荐使用Python 3.10，避免3.14的兼容性问题
2. **PyTorch版本**：必须安装CUDA版本，CPU版本无法使用GPU
3. **CUDA版本**：推荐CUDA 11.8，兼容性最佳

---

## 第二步：如何为情感咨询聊天机器人选择合适的模型

### 本步目标

根据你的硬件条件和应用需求，选择一个合适的大语言模型作为聊天机器人的基础模型。

### 第一步：分析你的硬件约束

#### 1.1 确定部署平台

| 部署平台 | 内存限制 | 推荐模型大小 |
|----------|----------|--------------|
| Orange Pi 5 (8GB) | 8GB | 量化后 <2.5GB |
| 树莓派 5 (8GB) | 8GB | 量化后 <2.5GB |
| 个人电脑 (16GB) | 16GB | 量化后 <4GB |

#### 1.2 考虑多模型并行

如果你的聊天机器人需要语音功能，需要同时运行多个模型：

```
总内存 = LLM + STT(语音转文字) + TTS(文字转语音) + 系统
```

**示例计算（Orange Pi 5 8GB）：**

| 模型 | 内存占用 |
|------|----------|
| LLM（聊天） | ~2.3GB |
| STT（Whisper-small） | ~1GB |
| TTS（Piper） | ~0.5GB |
| 系统+运行时 | ~2GB |
| **总计** | ~5.8GB ✅ 可行 |

### 第二步：了解模型大小与显存的关系

| 参数量 | FP16大小 | INT8量化 | INT4量化 |
|--------|----------|----------|----------|
| 1.5B | ~3GB | ~1.5GB | ~1GB |
| 3B | ~6GB | ~3GB | ~2GB |
| 7B | ~14GB | ~7GB | ~4GB |

### 第三步：候选模型对比

| 模型 | 参数量 | INT4大小 | 中文能力 | 发布时间 |
|------|--------|----------|----------|----------|
| **Qwen3-1.7B** | 1.7B | ~1.2GB | ⭐⭐⭐⭐⭐ | 2025年4月 |
| **Qwen3-4B** | 4B | ~2.3GB | ⭐⭐⭐⭐⭐ | 2025年4月 |
| Qwen2.5-1.5B | 1.5B | ~1GB | ⭐⭐⭐⭐⭐ | 2024年9月 |
| Gemma-2-2B | 2B | ~1.5GB | ⭐⭐⭐ | 2024年 |
| Phi-3.5-mini | 3.8B | ~2.5GB | ⭐⭐⭐ | 2024年 |

### 第四步：最终选择

**选择：Qwen3-1.7B（训练）+ Qwen3-4B（直接部署）**

理由：
1. 最新一代模型（2025年4月）
2. 中文能力最强
3. 支持思维链推理
4. 响应速度快

### 第五步：下载模型

从ModelScope下载模型（国内速度更快）：

**创建下载脚本 `download_model.py`：**

```python
from modelscope import snapshot_download
import os

print("开始下载 Qwen3-1.7B 模型")

model_dir = snapshot_download(
    'Qwen/Qwen3-1.7B',
    cache_dir='./models'
)

print(f"模型已下载到: {model_dir}")

# 列出下载的文件
for root, dirs, files in os.walk(model_dir):
    for file in files:
        filepath = os.path.join(root, file)
        size = os.path.getsize(filepath) / (1024 * 1024)
        print(f"  - {file}: {size:.2f} MB")
```

**运行下载脚本：**

```powershell
pip install modelscope
python download_model.py
```

模型将下载到 `models/Qwen/Qwen3-1.7B` 目录。

---

## 第三步：如何选择情感咨询训练数据集

### 本步目标

为你的情感咨询聊天机器人选择合适的训练数据集。

### 数据集策略

由于公开的情感咨询数据集存在以下问题：
- 语言不匹配（多为英文）
- 格式不统一
- 内容与目标场景不符

我们采用**自生成+清洗公开数据**的策略。

### 3.1 自生成数据集

#### CBT数据集（500条）

针对空巢老人情感需求，使用LLM生成CBT对话数据：

**CBT核心技术：**
- 认知重构：帮助用户识别和改变消极思维
- 行为激活：鼓励积极行为
- 证据检验：检验想法的真实性
- 苏格拉底式提问：引导用户自我发现

#### DBT数据集（500条）

针对空巢老人场景，生成DBT对话数据：

**DBT四大技能模块：**
1. 正念（Mindfulness）：活在当下
2. 情绪调节（Emotion Regulation）：管理强烈情绪
3. 人际效能（Interpersonal Effectiveness）：改善人际关系
4. 痛苦耐受（Distress Tolerance）：应对危机

### 3.2 清洗公开数据集

#### PsyDTCorpus

心理咨询对话数据集，需要清洗后使用：
- 提取有效对话
- 过滤无效内容
- 统一格式

#### catch_mdpcot

包含思维过程的数据集，需要特殊处理：
- 问题：包含"hoden"思维过程标记
- 解决：提取思维过程后的真实回复

---

## 第四步：数据集生成与清洗

### 本步目标

生成针对空巢老人的CBT/DBT数据集，并清洗合并其他数据集。

### 4.1 生成CBT数据集

针对空巢老人情感需求，生成500条CBT对话数据：

**CBT核心技术：**
- 认知重构
- 行为激活
- 证据检验
- 苏格拉底式提问

### 4.2 生成DBT数据集

针对空巢老人场景，生成500条DBT对话数据：

**DBT四大技能模块：**
1. 正念（Mindfulness）
2. 情绪调节（Emotion Regulation）
3. 人际效能（Interpersonal Effectiveness）
4. 痛苦耐受（Distress Tolerance）

### 4.3 数据集清洗

#### 清洗PsyDTCorpus

```python
# 提取有效对话
def clean_psydtcorpus(data):
    cleaned = []
    for item in data:
        if is_valid_dialog(item):
            cleaned.append(format_dialog(item))
    return cleaned
```

#### 清洗catch_mdpcot

该数据集包含大量"hoden"思维过程标记，需要提取真实回复：

```python
def extract_real_response(output):
    if "hoden" in output:
        parts = output.split("hoden", 1)
        if len(parts) >= 2:
            after_thinking = parts[1].strip()
            # 查找第一个空行后的内容
            double_newline_pos = after_thinking.find("\n\n")
            if double_newline_pos != -1:
                real_response = after_thinking[double_newline_pos:].strip()
                if len(real_response) >= 10:
                    return real_response
    return None
```

### 4.4 数据集合并

最终合并数据集统计：

| 来源 | 数量 |
|------|------|
| CBT数据 | 500条 |
| DBT数据 | 500条 |
| PsyDTCorpus（清洗后） | ~2,000条 |
| catch_mdpcot（清洗后） | ~3,700条 |
| **总计** | **6,734条** |

### 4.5 数据格式

采用ShareGPT格式：

```json
{
  "conversations": [
    {"role": "user", "content": "用户问题"},
    {"role": "assistant", "content": "助手回复"}
  ]
}
```

---

## 第五步：模型训练

### 本步目标

使用LLaMA-Factory对Qwen3-1.7B进行LoRA微调训练。

### 5.1 准备数据集配置

创建 `datasets/dataset_info.json`：

```json
{
  "dbt_counseling": {
    "file_name": "train_final.json",
    "formatting": "sharegpt",
    "columns": {
      "messages": "conversations"
    },
    "tags": {
      "role_tag": "role",
      "content_tag": "content",
      "user_tag": "user",
      "assistant_tag": "assistant"
    }
  }
}
```

### 5.2 创建训练配置

创建 `train.yaml`：

```yaml
model_name_or_path: models/Qwen/Qwen3-1___7B
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

output_dir: output/qwen3_dbt_lora
overwrite_output_dir: true

dataset: dbt_counseling
dataset_dir: datasets
template: qwen
cutoff_len: 1024
preprocessing_num_workers: 1

per_device_train_batch_size: 1
gradient_accumulation_steps: 8
lr_scheduler_type: cosine
logging_steps: 10
save_steps: 500
learning_rate: 5.0e-5
num_train_epochs: 3.0
max_grad_norm: 1.0
fp16: true

lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1

warmup_ratio: 0.1
report_to: none
```

### 5.3 启动训练

```powershell
py -3.10 -m llamafactory.cli train train.yaml
```

### 5.4 训练参数说明

| 参数 | 值 | 说明 |
|------|-----|------|
| per_device_train_batch_size | 1 | 单GPU批次大小 |
| gradient_accumulation_steps | 8 | 梯度累积步数 |
| learning_rate | 5e-5 | 学习率 |
| num_train_epochs | 3 | 训练轮数 |
| lora_rank | 8 | LoRA秩 |
| fp16 | true | 半精度训练 |

### 5.5 训练监控

训练过程中会输出Loss值：

```
{'loss': 3.7714, 'epoch': 0.01}
{'loss': 3.624, 'epoch': 0.04}
{'loss': 2.6813, 'epoch': 0.1}
{'loss': 2.27, 'epoch': 0.18}
```

Loss持续下降表示训练正常。

### 5.6 训练结果

| 指标 | 值 |
|------|-----|
| 训练轮数 | 3.0 ✅ |
| 最终Loss | 2.0131 |
| 训练时长 | 5小时57分16秒 |
| 总步数 | 2,526步 |

---

## 第六步：模型导出与部署

### 本步目标

将训练好的LoRA权重合并到基础模型，转换为GGUF格式，并部署到Ollama。

### 6.1 合并LoRA权重

创建合并配置 `merge.yaml`：

```yaml
model_name_or_path: models/Qwen/Qwen3-1___7B
adapter_name_or_path: output/qwen3_dbt_lora
template: qwen
finetuning_type: lora
export_dir: output/qwen3_dbt_merged
export_size: 2
export_device: cpu
export_legacy_format: false
```

执行合并：

```powershell
py -3.10 -m llamafactory.cli export merge.yaml
```

合并后的模型保存在 `output/qwen3_dbt_merged/` 目录。

### 6.2 安装llama.cpp

**方法一：下载源码**

从GitHub镜像下载llama.cpp源码，解压到项目目录。

**方法二：手动下载**

访问 https://ghproxy.com/https://github.com/ggerganov/llama.cpp/archive/refs/heads/master.zip

**安装依赖**：

```powershell
pip install -r llama.cpp-master/requirements.txt
```

### 6.3 转换为GGUF格式

```powershell
py -3.10 llama.cpp-master/convert_hf_to_gguf.py output/qwen3_dbt_merged --outfile output/dbt-counselor-f16.gguf --outtype f16
```

**转换结果**：

| 文件 | 大小 |
|------|------|
| `dbt-counselor-f16.gguf` | 3.4GB |

### 6.4 创建Ollama Modelfile

创建 `output/Modelfile`：

```
FROM ./dbt-counselor-f16.gguf

TEMPLATE """{{- if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{- end }}
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""

PARAMETER stop "<|im_end|>"
PARAMETER stop "<|im_start|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40

SYSTEM """你是一位专业的DBT（辩证行为疗法）心理咨询师，专门为空巢老人提供情感支持和心理疏导。你具备以下专业技能：
1. 正念技能：帮助来访者关注当下，接纳自己的情绪
2. 情绪调节：帮助识别和管理情绪
3. 人际效能：改善人际关系和沟通
4. 痛苦耐受：帮助应对困难和痛苦

请用温暖、理解、专业的态度与来访者交流。"""
```

### 6.5 导入Ollama

```powershell
ollama create dbt-counselor -f output/Modelfile
```

### 6.6 测试模型

```powershell
ollama run dbt-counselor
```

---

## 第七步：模型量化

### 本步目标

将FP16模型量化为Q4_K_M格式，大幅减少内存占用，适合部署到资源受限的设备。

### 7.1 为什么需要量化？

| 格式 | 精度 | 模型大小 | 内存需求 | 质量损失 |
|------|------|----------|----------|----------|
| FP16 | 16-bit | 100% | 高 | 0% |
| Q8_0 | 8-bit | 50% | 中 | ~1% |
| **Q4_K_M** | 4-bit | **35%** | **低** | **~3-5%** |

**Q4_K_M量化**：模型大小减少约65%，质量损失仅3-5%，是性价比最高的选择。

### 7.2 安装C++编译环境

量化需要编译llama.cpp工具，需要先安装C++编译环境。

#### 安装Visual Studio Build Tools

1. 访问：https://visualstudio.microsoft.com/visual-cpp-build-tools/
2. 下载 **"Visual Studio 生成工具"**
3. 安装时勾选 **"使用C++的桌面开发"**

#### 安装CMake

1. 访问：https://cmake.org/download/
2. 下载 **cmake-4.0.0-windows-x86_64.msi**
3. 安装时勾选 **"Add CMake to system PATH"**

#### 验证安装

```powershell
# 打开 Developer Command Prompt for VS 2022
where cl
where cmake
```

### 7.3 编译llama.cpp

```powershell
cd llama.cpp-master
mkdir build
cd build
cmake ..
cmake --build . --config Release
```

编译完成后，工具位于 `build\bin\Debug\` 目录。

### 7.4 量化模型

**步骤1：转换为GGUF（如果还没有）**

```powershell
python convert_hf_to_gguf.py output/qwen3_dbt_merged --outfile output/dbt-counselor-f16.gguf --outtype f16
```

**步骤2：量化为Q4_K_M**

```powershell
cd llama.cpp-master\build\bin\Debug
llama-quantize.exe C:\path\to\dbt-counselor-f16.gguf C:\path\to\dbt-counselor-q4_k_m.gguf Q4_K_M
```

**量化结果**：

| 模型 | FP16大小 | Q4_K_M大小 | 压缩比 |
|------|----------|------------|--------|
| Qwen3-1.7B | 3.4GB | 1.2GB | 65% |
| Qwen3-4B | 7.5GB | 2.3GB | 69% |

### 7.5 更新Modelfile使用量化模型

```
FROM ./dbt-counselor-q4_k_m.gguf
```

重新创建Ollama模型：

```powershell
ollama create dbt-counselor -f Modelfile
```

---

## 第八步：可选方案 - 训练更大的模型

### 本步目标

如果你的显卡显存更大（如RTX 4090 24GB），可以尝试训练更大的模型获得更好的效果。

### 8.1 显存需求参考

| 模型 | FP16训练 | QLoRA训练 | 推荐显卡 |
|------|----------|-----------|----------|
| Qwen3-1.7B | ~4-5GB | ~3GB | RTX 3060 (6GB) ✅ |
| Qwen3-4B | ~10-12GB | ~4-5GB | RTX 4070 (12GB) |
| Qwen3-8B | ~20GB | ~8GB | RTX 4090 (24GB) |

### 8.2 QLoRA训练（需要bitsandbytes）

**注意**：QLoRA需要bitsandbytes库，但该库在Windows上支持有限。

**配置文件 `train_4b_qlora.yaml`**：

```yaml
model_name_or_path: models/Qwen/Qwen3-4B
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

output_dir: output/qwen3_4b_dbt_lora

dataset: dbt_counseling
dataset_dir: datasets
template: qwen

per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 5.0e-5
num_train_epochs: 3.0

quantization_bit: 4
quantization_type: nf4

lora_rank: 8
lora_alpha: 16
```

**运行训练**：

```powershell
pip install bitsandbytes
llamafactory-cli train train_4b_qlora.yaml
```

### 8.3 Windows上的限制

| 问题 | 说明 |
|------|------|
| bitsandbytes | Windows不支持GPU版本 |
| DeepSpeed | Windows兼容性差 |
| 解决方案 | 使用WSL2/Linux或云服务器 |

### 8.4 替代方案：直接部署未微调的4B模型

如果无法训练4B模型，可以直接部署未微调的Qwen3-4B：

**下载并量化**：

```powershell
# 下载模型
python download_qwen3_4b.py

# 转换为GGUF
python convert_hf_to_gguf.py models/Qwen/Qwen3-4B --outfile output/qwen3-4b-f16.gguf --outtype f16

# 量化
llama-quantize.exe output/qwen3-4b-f16.gguf output/qwen3-4b-q4_k_m.gguf Q4_K_M
```

**创建Modelfile**：

```
FROM ./qwen3-4b-q4_k_m.gguf

TEMPLATE """{{- if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{- end }}
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""

SYSTEM """你是一位专业的DBT心理咨询师..."""
```

**模型对比**：

| 模型 | 大小 | DBT微调 | 回复质量 |
|------|------|----------|----------|
| Qwen3-1.7B (微调) | 1.2GB | ✅ 有 | 一般 |
| **Qwen3-4B (未微调)** | **2.3GB** | ❌ 无 | **更好** |

**结论**：即使未经微调，4B模型的回复质量也优于微调后的1.7B模型。

---

## 附录：常见问题与解决方案

### Q1: Python版本兼容性问题

**问题**：Python 3.14运行时报错 `TypeError: Pickler._batch_setitems()`

**解决**：使用Python 3.10

```powershell
py -3.10 -m venv .venv310
.venv310\Scripts\activate
```

### Q2: CUDA不可用

**问题**：`torch.cuda.is_available()` 返回 `False`

**解决**：安装CUDA版本的PyTorch

```powershell
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### Q3: 如何区分PyTorch版本

```powershell
pip show torch | findstr Version
```

输出示例：
- `Version: 2.5.1+cpu` → CPU版本
- `Version: 2.5.1+cu118` → CUDA 11.8版本

### Q4: 模板不存在错误

**问题**：`ValueError: Template qwen3_nothink does not exist`

**解决**：使用 `template: qwen` 而非 `qwen3_nothink`

### Q5: 数据集编码错误

**问题**：`'gbk' codec can't decode byte`

**解决**：将数据集文件名改为英文

### Q6: 训练4B模型失败

**问题**：RTX 3060 (6GB) 无法训练Qwen3-4B

**原因**：
- 显存不足（需要10GB+）
- bitsandbytes不支持Windows
- DeepSpeed不支持Windows

**解决方案**：
- 使用WSL2/Linux
- 租用云服务器
- 直接部署未微调的4B模型

---

## 第一阶段总结

### 完成的工作

| 阶段 | 状态 |
|------|------|
| 环境搭建 | ✅ 完成 |
| 模型选型 | ✅ 完成 |
| 数据集准备 | ✅ 完成 |
| 模型训练（1.7B） | ✅ 完成 |
| 模型导出 | ✅ 完成 |
| Ollama部署 | ✅ 完成 |
| 模型量化 | ✅ 完成 |
| 4B模型部署 | ✅ 完成 |

### 最终模型

| 模型 | 大小 | DBT微调 | 回复质量 | 推荐场景 |
|------|------|----------|----------|----------|
| dbt-counselor (1.7B) | 1.2GB | ✅ 有 | 一般 | 显存受限场景 |
| **qwen3-4b-counselor** | **2.3GB** | ❌ 无 | **更好** | **开发板部署** |

### 部署说明

#### 适用平台

本文档中的 GGUF 量化模型可通过 Ollama 部署在以下平台：

| 平台 | 支持情况 |
|------|----------|
| 树莓派 5 (8GB) | ✅ 支持 |
| Orange Pi 5 (RK3588) | ✅ 支持 |
| 普通 Linux 服务器 | ✅ 支持 |
| Windows/Mac | ✅ 支持 |

#### 昇腾NPU说明

对于搭载华为昇腾NPU的开发板（如 Orange Pi AI Pro），GGUF 格式**无法直接使用NPU加速**：

| 问题 | 说明 |
|------|------|
| 格式不兼容 | GGUF 是 llama.cpp 专用格式，昇腾NPU不支持 |
| 需要转换 | 需要将模型转换为 ONNX → INT8 格式 |
| 软件栈 | 需要使用华为 CANN + MindIE 软件栈 |

**昇腾NPU部署方案将在后续阶段研究。**

### 下一阶段预告

- 部署到Orange Pi 5
- 添加语音识别（STT）
- 添加语音合成（TTS)
- 集成完整聊天机器人系统

---

**版权所有 © L定远**

---

**版权所有 © L定远**
