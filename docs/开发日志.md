# å¼€å‘æ—¥å¿— - DBTæƒ…æ„Ÿå’¨è¯¢èŠå¤©æœºå™¨äºº

> **ç¼–å†™ï¼šEmma** | **å®¡æ ¸ï¼šMike** | **å›¢é˜Ÿï¼šç±³é†‹ç”µå­å·¥ä½œå®¤**
>
> **é¡¹ç›®ç›®æ ‡**ï¼šè®­ç»ƒä¸€ä¸ªå…·æœ‰DBTæƒ…æ„Ÿå’¨è¯¢èƒ½åŠ›çš„èŠå¤©æœºå™¨äººï¼Œéƒ¨ç½²åœ¨Orange Pi 5ä¸Š

---

## 2026-02-14

### ä»»åŠ¡ï¼šè®­ç»ƒç¯å¢ƒæ­å»º

### å®Œæˆçš„å·¥ä½œ

1. **ç¯å¢ƒæ£€æµ‹**
   - æ£€æµ‹åˆ°ç³»ç»ŸPython 3.9.2
   - æ£€æµ‹åˆ°PyCharmè™šæ‹Ÿç¯å¢ƒ `.venv`ï¼ˆPython 3.14ï¼‰
   - å‘ç°å·²å®‰è£…çš„åº“ï¼štorch 2.5.1+cu121, torchvision, torchaudio

2. **ä¾èµ–åº“æ•´ç†**
   - æ•´ç†å‡ºéœ€è¦å®‰è£…çš„æ ¸å¿ƒåº“æ¸…å•
   - ç”¨æˆ·åœ¨PyCharmä¸­å®Œæˆäº†è¿™äº›åº“çš„å®‰è£…

3. **è®­ç»ƒæ¡†æ¶å®‰è£…**
   - ç¡®è®¤ç”¨æˆ·å®‰è£…äº†LLaMA-Factoryè®­ç»ƒæ¡†æ¶
   - ç¡®è®¤ç”¨æˆ·å®‰è£…äº†UnslothåŠ é€Ÿåº“

### å®‰è£…çš„ä¾èµ–åŒ…

| åŒ…å | è¯´æ˜ |
|------|------|
| torch | PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶ |
| transformers | Hugging Face transformersåº“ |
| sentencepiece | æ–‡æœ¬åˆ†è¯å·¥å…· |
| gradio | Webç•Œé¢æ¡†æ¶ |
| llamafactory | LLaMA-Factoryè®­ç»ƒæ¡†æ¶ |
| unsloth | è®­ç»ƒåŠ é€Ÿåº“ |

---

## 2026-02-15

### ä»»åŠ¡ï¼šæ¨¡å‹é€‰å‹ä¸æ•°æ®é›†è°ƒç ”

### å®Œæˆçš„å·¥ä½œ

1. **éƒ¨ç½²ç¯å¢ƒåˆ†æ**
   - Orange Pi 5 å†…å­˜ï¼š8GB
   - éœ€åŒæ—¶è¿è¡Œï¼šLLM + STTï¼ˆè¯­éŸ³è½¬æ–‡å­—ï¼‰+ TTSï¼ˆæ–‡å­—è½¬è¯­éŸ³ï¼‰

2. **æ¨¡å‹é€‰å‹å¯¹æ¯”**

   | æ¨¡å‹ | å‚æ•° | é‡åŒ–åå¤§å° | ä¸­æ–‡èƒ½åŠ› | æ¨èåº¦ |
   |------|------|------------|----------|--------|
   | **Qwen3-1.7B** | 1.7B | ~1.2GB | â­â­â­â­â­ | â­é¦–é€‰ |
   | Qwen2.5-1.5B | 1.5B | ~1GB | â­â­â­â­â­ | â­â­ |
   | Gemma-2-2B | 2B | ~1.5GB | â­â­â­ | â­ |

3. **æœ€ç»ˆé€‰æ‹©ï¼šQwen3-1.7B**

4. **æ•°æ®é›†è°ƒç ”**
   - ESConv - æƒ…æ„Ÿæ”¯æŒå¯¹è¯æ•°æ®é›†
   - PsyQA - ä¸­æ–‡å¿ƒç†å¥åº·é—®ç­”æ•°æ®é›†

---

## 2026-02-16

### ä»»åŠ¡ï¼šæ•°æ®é›†ç”Ÿæˆã€æ¸…æ´—ä¸è®­ç»ƒå¯åŠ¨

### å®Œæˆçš„å·¥ä½œ

#### 1. æ•°æ®é›†ç”Ÿæˆ

**ç”ŸæˆCBTæ•°æ®é›†ï¼ˆ500æ¡ï¼‰**ï¼š
- é’ˆå¯¹ç©ºå·¢è€äººæƒ…æ„Ÿéœ€æ±‚
- åŒ…å«è®¤çŸ¥é‡æ„ã€è¡Œä¸ºæ¿€æ´»ç­‰æŠ€æœ¯
- æ ¼å¼ï¼šShareGPTå¯¹è¯æ ¼å¼

**ç”ŸæˆDBTæ•°æ®é›†ï¼ˆ500æ¡ï¼‰**ï¼š
- å››å¤§æŠ€èƒ½æ¨¡å—ï¼šæ­£å¿µã€æƒ…ç»ªè°ƒèŠ‚ã€äººé™…æ•ˆèƒ½ã€ç—›è‹¦è€å—
- é’ˆå¯¹ç©ºå·¢è€äººåœºæ™¯
- æ ¼å¼ï¼šShareGPTå¯¹è¯æ ¼å¼

#### 2. æ•°æ®é›†æ¸…æ´—ä¸åˆå¹¶

**å¤„ç†PsyDTCorpusæ•°æ®é›†**ï¼š
- åŸå§‹æ•°æ®ï¼šå¿ƒç†å’¨è¯¢å¯¹è¯
- æ¸…æ´—ç­–ç•¥ï¼šæå–æœ‰æ•ˆå¯¹è¯ï¼Œè¿‡æ»¤æ— æ•ˆå†…å®¹
- æœ€ç»ˆä¿ç•™ï¼šé«˜è´¨é‡å¿ƒç†å’¨è¯¢å¯¹è¯

**å¤„ç†catch_mdpcotæ•°æ®é›†**ï¼š
- é—®é¢˜ï¼šåŒ…å«å¤§é‡"hoden"æ€ç»´è¿‡ç¨‹æ ‡è®°
- è§£å†³æ–¹æ¡ˆï¼šå¼€å‘ä¸“ç”¨æ¸…æ´—å‡½æ•°æå–çœŸå®å›å¤
- æ¸…æ´—å‡½æ•°æ ¸å¿ƒé€»è¾‘ï¼š
  ```python
  def extract_real_response(output):
      if "hoden" in output:
          parts = output.split("hoden", 1)
          # æå–æ€ç»´è¿‡ç¨‹åçš„çœŸå®å›å¤
          ...
  ```

**æœ€ç»ˆåˆå¹¶æ•°æ®é›†**ï¼š
- æ€»æ ·æœ¬æ•°ï¼š**6,734æ¡**
- æ¥æºï¼šCBTæ•°æ® + DBTæ•°æ® + æ¸…æ´—åçš„PsyDTCorpus + æ¸…æ´—åçš„catch_mdpcot
- æ ¼å¼ï¼šShareGPTæ ¼å¼ï¼ˆconversationså­—æ®µï¼‰

#### 3. è®­ç»ƒç¯å¢ƒé…ç½®

**é‡åˆ°çš„é—®é¢˜**ï¼š

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| Python 3.14å…¼å®¹æ€§ | dillåº“ä¸å…¼å®¹ | åˆ›å»ºPython 3.10ç¯å¢ƒ |
| PyTorch CPUç‰ˆæœ¬ | æ— æ³•ä½¿ç”¨GPU | å®‰è£…CUDA 11.8ç‰ˆæœ¬ |
| æ¨¡æ¿ä¸å­˜åœ¨ | qwen3_nothinkæ¨¡æ¿æœªæ”¯æŒ | æ”¹ç”¨qwenæ¨¡æ¿ |

**ç¯å¢ƒé…ç½®æ­¥éª¤**ï¼š

1. åˆ›å»ºPython 3.10è™šæ‹Ÿç¯å¢ƒ
   ```powershell
   py -3.10 -m venv .venv310
   ```

2. å®‰è£…PyTorch CUDAç‰ˆæœ¬
   ```powershell
   # ä¸‹è½½torch-2.5.1+cu118-cp310-cp310-win_amd64.whl
   py -3.10 -m pip install torch-2.5.1+cu118-cp310-cp310-win_amd64.whl
   ```

3. éªŒè¯CUDA
   ```
   PyTorchç‰ˆæœ¬: 2.5.1+cu118
   CUDAå¯ç”¨: True
   GPUè®¾å¤‡: NVIDIA GeForce RTX 3060 Laptop GPU
   ```

#### 4. è®­ç»ƒé…ç½®

**train.yamlé…ç½®**ï¼š
```yaml
model_name_or_path: models/Qwen/Qwen3-1___7B
stage: sft
finetuning_type: lora
lora_target: all

dataset: dbt_counseling
dataset_dir: datasets
template: qwen
cutoff_len: 1024

per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 5.0e-5
num_train_epochs: 3.0
fp16: true

lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1
```

#### 5. è®­ç»ƒå¯åŠ¨

**è®­ç»ƒçŠ¶æ€**ï¼š
- è®­ç»ƒæ¡†æ¶ï¼šLLaMA-Factory
- è®­ç»ƒæ–¹æ³•ï¼šLoRAå¾®è°ƒ
- æ•°æ®é›†ï¼š6,734æ¡
- æ€»æ­¥æ•°ï¼š2,526æ­¥
- å½“å‰è¿›åº¦ï¼š7%ï¼ˆ179/2526æ­¥ï¼‰
- é¢„è®¡æ—¶é—´ï¼šçº¦6å°æ—¶

**Lossä¸‹é™è¶‹åŠ¿**ï¼š
| æ­¥æ•° | Loss |
|------|------|
| 10 | 3.77 |
| 50 | 3.23 |
| 100 | 2.68 |
| 179 | 2.27 |

è®­ç»ƒæ­£å¸¸è¿›è¡Œä¸­ âœ…

### ç”Ÿæˆçš„æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `datasets/train_final.json` | æœ€ç»ˆè®­ç»ƒæ•°æ®é›†ï¼ˆ6,734æ¡ï¼‰ |
| `datasets/dataset_info.json` | LLaMA-Factoryæ•°æ®é›†é…ç½® |
| `train.yaml` | è®­ç»ƒé…ç½®æ–‡ä»¶ |
| `check_cuda.py` | CUDAéªŒè¯è„šæœ¬ |

---

## 2026-02-17

### ä»»åŠ¡ï¼šè®­ç»ƒå®Œæˆä¸æ¨¡å‹éƒ¨ç½²

### å®Œæˆçš„å·¥ä½œ

#### 1. è®­ç»ƒå®Œæˆ

**è®­ç»ƒç»“æœ**ï¼š

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| è®­ç»ƒè½®æ•° | 3.0 âœ… |
| æœ€ç»ˆLoss | 2.0131 |
| è®­ç»ƒæ—¶é•¿ | 5å°æ—¶57åˆ†16ç§’ |
| æ€»æ­¥æ•° | 2,526æ­¥ |
| æ¨¡å‹ä¿å­˜ä½ç½® | `output/qwen3_dbt_lora` |

**Losså˜åŒ–è¶‹åŠ¿**ï¼š

| é˜¶æ®µ | Loss |
|------|------|
| å¼€å§‹ | 3.77 |
| ä¸­æœŸ | 2.68 |
| ç»“æŸ | 2.01 |

Lossä»3.77é™åˆ°2.01ï¼Œè®­ç»ƒæ•ˆæœè‰¯å¥½ âœ…

#### 2. LoRAæƒé‡åˆå¹¶

**åˆå¹¶é…ç½® `merge.yaml`**ï¼š

```yaml
model_name_or_path: models/Qwen/Qwen3-1___7B
adapter_name_or_path: output/qwen3_dbt_lora
template: qwen
finetuning_type: lora
export_dir: output/qwen3_dbt_merged
export_size: 2
export_device: cpu
export_legacy_format: false
```

**æ‰§è¡Œåˆå¹¶**ï¼š

```powershell
py -3.10 -m llamafactory.cli export merge.yaml
```

**åˆå¹¶ç»“æœ**ï¼š

| é¡¹ç›® | çŠ¶æ€ |
|------|------|
| åˆå¹¶æº | `output/qwen3_dbt_lora` âœ… |
| åˆå¹¶ç›®æ ‡ | `output/qwen3_dbt_merged` âœ… |
| æ¨¡å‹å¤§å° | ~3.4GBï¼ˆåˆ†2ä¸ªæ–‡ä»¶ï¼‰ |

#### 3. GGUFæ ¼å¼è½¬æ¢

**å®‰è£…llama.cppä¾èµ–**ï¼š

```powershell
pip install -r llama.cpp-master/requirements.txt
```

**è½¬æ¢ä¸ºGGUF**ï¼š

```powershell
py -3.10 llama.cpp-master/convert_hf_to_gguf.py output/qwen3_dbt_merged --outfile output/dbt-counselor-f16.gguf --outtype f16
```

**è½¬æ¢ç»“æœ**ï¼š

| æ–‡ä»¶ | å¤§å° |
|------|------|
| `dbt-counselor-f16.gguf` | 3.4GB âœ… |

#### 4. Ollamaéƒ¨ç½²

**åˆ›å»ºModelfile**ï¼š

```
FROM ./dbt-counselor-f16.gguf

TEMPLATE """{{- if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{- end }}
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""

PARAMETER stop "<|im_end|>"
PARAMETER stop "<|im_start|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40

SYSTEM """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„DBTå¿ƒç†å’¨è¯¢å¸ˆ..."""
```

**å¯¼å…¥Ollama**ï¼š

```powershell
ollama create dbt-counselor -f output/Modelfile
```

#### 5. æ¨¡å‹æµ‹è¯•

**æµ‹è¯•ç»“æœ**ï¼š

| æµ‹è¯•é—®é¢˜ | å›å¤è´¨é‡ |
|----------|----------|
| å­¤ç‹¬æ„Ÿå’¨è¯¢ | âœ… æ­£å¸¸ |
| æƒ…æ„Ÿæ”¯æŒ | âœ… æ­£å¸¸ |
| å±æœºå¹²é¢„ | âš ï¸ éœ€ä¼˜åŒ– |

---

## 2026-02-18

### ä»»åŠ¡ï¼šå°è¯•è®­ç»ƒQwen3-4Bæ¨¡å‹ä¸æ¨¡å‹é‡åŒ–

### å®Œæˆçš„å·¥ä½œ

#### 1. ä¸‹è½½Qwen3-4Bæ¨¡å‹

**ä¸‹è½½è„šæœ¬ `download_qwen3_4b.py`**ï¼š

```python
from modelscope import snapshot_download

model_dir = snapshot_download(
    'Qwen/Qwen3-4B',
    cache_dir='./models'
)
```

**ä¸‹è½½ç»“æœ**ï¼š

| é¡¹ç›® | å€¼ |
|------|-----|
| æ¨¡å‹è·¯å¾„ | `models/Qwen/Qwen3-4B` |
| æ¨¡å‹å¤§å° | ~8GB |

#### 2. å°è¯•è®­ç»ƒ4Bæ¨¡å‹ï¼ˆå¤±è´¥ï¼‰

**å°è¯•æ–¹æ¡ˆ**ï¼š

| æ–¹æ¡ˆ | è¯´æ˜ | ç»“æœ |
|------|------|------|
| FP16è®­ç»ƒ | ç›´æ¥è®­ç»ƒ | âŒ æ˜¾å­˜ä¸è¶³ |
| QLoRA 4-bit | é‡åŒ–è®­ç»ƒ | âŒ bitsandbytesä¸æ”¯æŒWindows |
| DeepSpeed ZeRO-2 | CPU Offload | âŒ DeepSpeedä¸æ”¯æŒWindows |
| ä½æ˜¾å­˜é…ç½® | Gradient Checkpointing | âŒ ä»ç„¶ä¸è¶³ |

**å¤±è´¥åŸå› åˆ†æ**ï¼š

| é—®é¢˜ | è¯´æ˜ |
|------|------|
| RTX 3060æ˜¾å­˜ | åªæœ‰6GBï¼Œ4Bæ¨¡å‹éœ€è¦~10GB |
| bitsandbytes | Windowsä¸æ”¯æŒGPUç‰ˆæœ¬ |
| DeepSpeed | Windowså…¼å®¹æ€§å·® |

**ç»“è®º**ï¼šRTX 3060 (6GB) æ— æ³•è®­ç»ƒQwen3-4Bï¼Œåªèƒ½è®­ç»ƒ1.7Bæ¨¡å‹ã€‚

#### 3. C++ç¯å¢ƒé…ç½®

**å®‰è£…Visual Studio Build Tools**ï¼š

- ç‰ˆæœ¬ï¼šVisual Studio 2022 Build Tools
- ç»„ä»¶ï¼šä½¿ç”¨C++çš„æ¡Œé¢å¼€å‘
- å¤§å°ï¼š~2.5GB

**å®‰è£…CMake**ï¼š

- ç‰ˆæœ¬ï¼šCMake 4.0.0
- é…ç½®ï¼šæ·»åŠ åˆ°ç³»ç»ŸPATH

**éªŒè¯å®‰è£…**ï¼š

```powershell
where cl
# è¾“å‡ºï¼šC:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.42.34433\bin\Hostx86\x86\cl.exe

where cmake
# è¾“å‡ºï¼šC:\Program Files\CMake\bin\cmake.exe
```

#### 4. ç¼–è¯‘llama.cpp

**ç¼–è¯‘æ­¥éª¤**ï¼š

```powershell
cd llama.cpp-master
mkdir build
cd build
cmake ..
cmake --build . --config Release
```

**ç¼–è¯‘ç»“æœ**ï¼š

| å·¥å…· | è·¯å¾„ | çŠ¶æ€ |
|------|------|------|
| llama-quantize.exe | `build\bin\Debug\llama-quantize.exe` | âœ… |
| llama-cli.exe | `build\bin\Debug\llama-cli.exe` | âœ… |
| llama-server.exe | `build\bin\Debug\llama-server.exe` | âœ… |

#### 5. æ¨¡å‹é‡åŒ–

**Qwen3-1.7Bé‡åŒ–**ï¼š

```powershell
# è½¬æ¢ä¸ºGGUF
python convert_hf_to_gguf.py output/qwen3_dbt_merged --outfile output/dbt-counselor-f16.gguf --outtype f16

# é‡åŒ–ä¸ºQ4_K_M
llama-quantize.exe output/dbt-counselor-f16.gguf output/dbt-counselor-q4_k_m.gguf Q4_K_M
```

**Qwen3-4Bé‡åŒ–**ï¼š

```powershell
# è½¬æ¢ä¸ºGGUF
python convert_hf_to_gguf.py models/Qwen/Qwen3-4B --outfile output/qwen3-4b-f16.gguf --outtype f16

# é‡åŒ–ä¸ºQ4_K_M
llama-quantize.exe output/qwen3-4b-f16.gguf output/qwen3-4b-q4_k_m.gguf Q4_K_M
```

**é‡åŒ–ç»“æœå¯¹æ¯”**ï¼š

| æ¨¡å‹ | FP16å¤§å° | Q4_K_Må¤§å° | å‹ç¼©æ¯” |
|------|----------|------------|--------|
| Qwen3-1.7B | 3.4GB | 1.2GB | 65% |
| Qwen3-4B | 7.5GB | 2.3GB | 69% |

#### 6. æ¨¡å‹æµ‹è¯•å¯¹æ¯”

**æµ‹è¯•é—®é¢˜ï¼šå­¤ç‹¬æ„Ÿå’¨è¯¢**

| æ¨¡å‹ | å›å¤é•¿åº¦ | ä¸“ä¸šæ€§ | å…±æƒ…èƒ½åŠ› |
|------|----------|--------|----------|
| Qwen3-1.7B (å¾®è°ƒ) | çŸ­ | ä¸€èˆ¬ | ä¸€èˆ¬ |
| **Qwen3-4B (æœªå¾®è°ƒ)** | **é•¿ä¸”è¯¦ç»†** | **å¼º** | **å¼º** |

**æµ‹è¯•é—®é¢˜ï¼šè‡ªæ€æ„å¿µ**

| æ¨¡å‹ | çƒ­çº¿è§¦å‘ | å›å¤è´¨é‡ |
|------|----------|----------|
| Qwen3-1.7B (å¾®è°ƒ) | âŒ æœªè§¦å‘ | ç®€å• |
| Qwen3-4B (æœªå¾®è°ƒ) | âš ï¸ éƒ¨åˆ†è§¦å‘ | è¯¦ç»†ä¸“ä¸š |

**ç»“è®º**ï¼šQwen3-4Bè™½ç„¶æœªç»å¾®è°ƒï¼Œä½†å›å¤è´¨é‡æ˜æ˜¾ä¼˜äºå¾®è°ƒåçš„1.7Bæ¨¡å‹ã€‚

#### 7. æœ€ç»ˆæ¨¡å‹é€‰æ‹©

| æ¨¡å‹ | å¤§å° | DBTå¾®è°ƒ | å›å¤è´¨é‡ | æ¨èåœºæ™¯ |
|------|------|----------|----------|----------|
| dbt-counselor (1.7B) | 1.2GB | âœ… æœ‰ | ä¸€èˆ¬ | æ˜¾å­˜å—é™åœºæ™¯ |
| **qwen3-4b-counselor** | **2.3GB** | âŒ æ—  | **æ›´å¥½** | **Orange Pi 5 (8GB)** |

### ç”Ÿæˆçš„æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `train_4b.yaml` | 4Bæ¨¡å‹è®­ç»ƒé…ç½® |
| `train_4b_qlora.yaml` | QLoRAè®­ç»ƒé…ç½® |
| `train_4b_deepspeed.yaml` | DeepSpeedè®­ç»ƒé…ç½® |
| `train_4b_lowmem.yaml` | ä½æ˜¾å­˜è®­ç»ƒé…ç½® |
| `ds_config.json` | DeepSpeedé…ç½® |
| `output/qwen3-4b-f16.gguf` | 4Bæ¨¡å‹FP16æ ¼å¼ |
| `output/qwen3-4b-q4_k_m.gguf` | 4Bæ¨¡å‹é‡åŒ–ç‰ˆ |
| `output/dbt-counselor-q4_k_m.gguf` | 1.7Bæ¨¡å‹é‡åŒ–ç‰ˆ |
| `output/Modelfile-4b` | 4Bæ¨¡å‹Ollamaé…ç½® |

### ä¸‹ä¸€æ­¥è®¡åˆ’

1. âœ… éƒ¨ç½²åˆ°Orange Pi 5
2. âœ… æ·»åŠ è¯­éŸ³åŠŸèƒ½ï¼ˆSTT/TTSï¼‰
3. â³ ä¼˜åŒ–å±æœºå¹²é¢„èƒ½åŠ›
4. â³ ç ”ç©¶æ˜‡è…¾NPUåŠ é€Ÿæ–¹æ¡ˆ
```

**éƒ¨ç½²æˆåŠŸ** âœ…

### ç”Ÿæˆçš„æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `output/qwen3_dbt_lora/` | LoRAæƒé‡ç›®å½• |
| `output/qwen3_dbt_merged/` | åˆå¹¶åæ¨¡å‹ç›®å½• |
| `output/dbt-counselor-f16.gguf` | GGUFæ ¼å¼æ¨¡å‹ |
| `output/Modelfile` | Ollamaé…ç½®æ–‡ä»¶ |
| `merge.yaml` | æ¨¡å‹åˆå¹¶é…ç½® |

### ä¸‹ä¸€æ­¥è®¡åˆ’

1. âœ… è®­ç»ƒå®Œæˆ
2. âœ… LoRAæƒé‡åˆå¹¶
3. âœ… GGUFæ ¼å¼è½¬æ¢
4. âœ… Ollamaéƒ¨ç½²æµ‹è¯•
5. â³ éƒ¨ç½²åˆ°Orange Pi 5
6. â³ æ·»åŠ è¯­éŸ³åŠŸèƒ½ï¼ˆSTT/TTSï¼‰

---

## é¡¹ç›®è¿›åº¦æ€»è§ˆ

| é˜¶æ®µ | çŠ¶æ€ | å®Œæˆåº¦ |
|------|------|--------|
| ç¯å¢ƒæ­å»º | âœ… å®Œæˆ | 100% |
| æ¨¡å‹é€‰å‹ | âœ… å®Œæˆ | 100% |
| æ•°æ®é›†å‡†å¤‡ | âœ… å®Œæˆ | 100% |
| æ¨¡å‹è®­ç»ƒ | ğŸ”„ è¿›è¡Œä¸­ | 7% |
| æ¨¡å‹å¯¼å‡º | â³ å¾…å¼€å§‹ | 0% |
| æ¨¡å‹éƒ¨ç½² | â³ å¾…å¼€å§‹ | 0% |

---

*æœ¬æ–‡æ¡£ç”±ç±³é†‹ç”µå­å·¥ä½œå®¤ç»´æŠ¤ï¼Œç‰ˆæƒæ‰€æœ‰*
